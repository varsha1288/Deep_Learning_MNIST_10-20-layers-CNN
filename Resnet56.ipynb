{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(shape):\n",
    "    '''\n",
    "    Weights initialization helper function.\n",
    "    \n",
    "    Input(s): shape - Type: int list, Example: [5, 5, 32, 32], This parameter is used to define dimensions of weights tensor\n",
    "    \n",
    "    Output: tensor of weights in shape defined with the input to this function\n",
    "    '''\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_init(shape, bias_value=0.01):\n",
    "    '''\n",
    "    Bias initialization helper function.\n",
    "    \n",
    "    Input(s): shape - Type: int list, Example: [32], This parameter is used to define dimensions of bias tensor.\n",
    "              bias_value - Type: float number, Example: 0.01, This parameter is set to be value of bias tensor.\n",
    "    \n",
    "    Output: tensor of biases in shape defined with the input to this function\n",
    "    '''\n",
    "    return tf.Variable(tf.constant(bias_value, shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_custom(input, filter_size, num_of_channels, num_of_filters, activation=tf.nn.relu, dropout=None,\n",
    "                  padding='SAME', max_pool=True, strides=(1, 1)):  \n",
    "    '''\n",
    "    This function is used to define a convolutional layer for a network,\n",
    "    \n",
    "    Input(s): input - this is input into convolutional layer (Previous layer or an image)\n",
    "              filter_size - also called kernel size, kernel is moved (convolved) across an image. Example: 3\n",
    "              number_of_channels - how many channels the input tensor has\n",
    "              number_of_filters - this is hyperparameter, and this will set one of dimensions of the output tensor from \n",
    "                                  this layer. Note: this number will be number_of_channels for the layer after this one\n",
    "              max_pool - if this is True, output tensor will be 2x smaller in size. Max pool is there to decrease spartial \n",
    "                        dimensions of our output tensor, so computation is less expensive.\n",
    "              padding - the way that we pad input tensor with zeros (\"SAME\" or \"VALID\")\n",
    "              activation - the non-linear function used at this layer.\n",
    "              \n",
    "              \n",
    "    Output: Convolutional layer with input parameters.\n",
    "    '''\n",
    "    weights = weights_init([filter_size, filter_size, num_of_channels, num_of_filters])\n",
    "    bias = bias_init([num_of_filters])\n",
    "    \n",
    "    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding=padding) + bias\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if max_pool:\n",
    "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    if dropout != None:\n",
    "        layer = tf.nn.dropout(layer, dropout)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(layer):\n",
    "    '''\n",
    "    This method is used to convert convolutional output (4 dimensional tensor) into 2 dimensional tensor.\n",
    "    \n",
    "    Input(s): layer - the output from last conv layer in your network (4d tensor)\n",
    "    \n",
    "    Output(s): reshaped - reshaped layer, 2 dimensional matrix\n",
    "               elements_num - number of features for this layer\n",
    "    '''\n",
    "    shape = layer.get_shape()\n",
    "    \n",
    "    num_elements_ = shape[1:4].num_elements()\n",
    "    \n",
    "    flattened_layer = tf.reshape(layer, [-1, num_elements_])\n",
    "    return flattened_layer, num_elements_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_custom(input, input_size, output_size, activation=tf.nn.relu, dropout=None):\n",
    "    '''\n",
    "    This function is used to define a fully connected layer for a network,\n",
    "    \n",
    "    Input(s): input - this is input into fully connected (Dense) layer (Previous layer or an image)\n",
    "              input_size - how many neurons/features the input tensor has. Example: input.shape[1]\n",
    "              output_shape - how many neurons this layer will have\n",
    "              activation - the non-linear function used at this layer.    \n",
    "              dropout - the regularization method used to prevent overfitting. The way it works, we randomly turn off\n",
    "                        some neurons in this layer\n",
    "              \n",
    "    Output: fully connected layer with input parameters.\n",
    "    '''\n",
    "    weights = weights_init([input_size, output_size])\n",
    "    bias = bias_init([output_size])\n",
    "    \n",
    "    layer = tf.matmul(input, weights) + bias\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if dropout != None:\n",
    "        layer = tf.nn.dropout(layer, dropout)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_unit(layer):\n",
    "    '''\n",
    "    Input(s): layer - conv layer before this res unit\n",
    "    \n",
    "    Output(s): ResUnit layer - implemented as described in the paper\n",
    "    '''\n",
    "    step1 = tf.layers.batch_normalization(layer)\n",
    "    step2 = tf.nn.relu(step1)\n",
    "    step3 = conv2d_custom(step2, 3, 32, 32, activation=None, max_pool=False) #32 number of feautres is hyperparam\n",
    "    step4 = tf.layers.batch_normalization(step3)\n",
    "    step5 = tf.nn.relu(step4)\n",
    "    step6 = conv2d_custom(step5, 3, 32, 32, activation=None, max_pool=False)\n",
    "    return layer + step6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, 28, 28, 1], name='inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, 10], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_layers = 20\n",
    "between_strides = num_of_layers/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev1 = conv2d_custom(inputs, 3, 1, 32, activation=None, max_pool=False)\n",
    "prev1 = tf.layers.batch_normalization(prev1)\n",
    "for i in range(5): # this number * between_strides = number_of_layers\n",
    "    for j in range(int(between_strides)):\n",
    "        prev1 = residual_unit(prev1)\n",
    "    #After 4 res units we perform strides 2x2, which will reduce data\n",
    "    perv1 = conv2d_custom(inputs, 3, 1, 32, activation=None, max_pool=False, strides=[2, 2])\n",
    "    prev1 = tf.layers.batch_normalization(prev1)\n",
    "#after all resunits we have last conv layer, than flattening and output layer\n",
    "last_conv = conv2d_custom(prev1, 3, 32, 10, activation=None, max_pool=False)\n",
    "flat, features = flatten(last_conv)\n",
    "output = dense_custom(flat, features, 10, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is for computing the accuracy of this model\n",
    "pred_y = tf.nn.softmax(output)\n",
    "pred_y_true = tf.argmax(pred_y, 1)\n",
    "y_true = tf.argmax(targets, 1)\n",
    "correct_prediction = tf.equal(pred_y_true, y_true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-b0fcd8e55918>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loss function and optimizer\n",
    "cost = tf.reduce_mean((tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=targets)))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "total_number_trained = 0\n",
    "epochs = 5\n",
    "def optmizer():\n",
    "\n",
    "    for i in (range(epochs)):\n",
    "        epoch_loss = []\n",
    "        start_epoch = time.time()\n",
    "        for ii in range(mnist_data.train.num_examples//batch_size):\n",
    "            batch = mnist_data.train.next_batch(batch_size)\n",
    "            imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "            labs = batch[1]\n",
    "\n",
    "            dict_input = {inputs:imgs, targets:labs}\n",
    "\n",
    "            c, _ = session.run([cost, optimizer], feed_dict=dict_input)\n",
    "            epoch_loss.append(c)\n",
    "        print(\"Epoche: {}/{}\".format(i+1, epochs), \"| Training accuracy: \", session.run(accuracy, feed_dict=dict_input), \n",
    "              \"| Cost: {}\".format(np.mean(epoch_loss)), \" | Time for epoch: {:.2f}s\".format(time.time() - start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_valid = 1000\n",
    "def validate_model():\n",
    "    accuracy_per_batch = []\n",
    "    for ii in range(mnist_data.validation.num_examples//batch_size_valid):\n",
    "        batch = mnist_data.validation.next_batch(batch_size_valid)\n",
    "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        labs = batch[1]\n",
    "\n",
    "        accuracy_per_batch.append(session.run(accuracy, feed_dict={inputs:imgs, targets:labs}))\n",
    "\n",
    "    print(\"Validation per batch accuracy {}\".format(accuracy_per_batch))\n",
    "    print(\"Test accuracy average: {:.2f}%\".format(np.mean(accuracy_per_batch)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_test = 1000\n",
    "def test_model():\n",
    "    accuracy_per_batch = []\n",
    "    for ii in range(mnist_data.test.num_examples//batch_size_test):\n",
    "        batch = mnist_data.test.next_batch(batch_size_test)\n",
    "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        labs = batch[1]\n",
    "\n",
    "        accuracy_per_batch.append(session.run(accuracy, feed_dict={inputs:imgs, targets:labs}))\n",
    "\n",
    "    print(\"Test per batch accuracy {}\".format(accuracy_per_batch))\n",
    "    print(\"Test accuracy average: {:.2f}%\".format(np.mean(accuracy_per_batch)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche: 1/5 | Training accuracy:  1.0 | Cost: 0.17186106741428375  | Time for epoch: 2610.14s\n",
      "Epoche: 2/5 | Training accuracy:  1.0 | Cost: 0.07193352282047272  | Time for epoch: 3545.89s\n",
      "Epoche: 3/5 | Training accuracy:  1.0 | Cost: 0.05727747082710266  | Time for epoch: 3279.79s\n",
      "Epoche: 4/5 | Training accuracy:  1.0 | Cost: 0.047146622091531754  | Time for epoch: 31251.41s\n",
      "Epoche: 5/5 | Training accuracy:  1.0 | Cost: 0.03967192769050598  | Time for epoch: 10836.92s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optmizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test per batch accuracy [0.988, 0.978, 0.988, 0.982, 0.984, 0.991, 0.981, 0.985, 0.994, 0.984]\n",
      "Test accuracy average: 98.55%\n"
     ]
    }
   ],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation per batch accuracy [0.988, 0.985, 0.989, 0.981, 0.986]\n",
      "Test accuracy average: 98.58%\n"
     ]
    }
   ],
   "source": [
    "validate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "#close the session after testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
